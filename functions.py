import streamlit as st
import os
import time
from pathlib import Path
import wave
import pyaudio
from pydub import AudioSegment
from audiorecorder import audiorecorder
import numpy as np
from scipy.io.wavfile import write
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage
from langchain.memory import ConversationSummaryBufferMemory
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
import constants as ct

def record_audio(audio_input_file_path):
    """
    éŸ³å£°å…¥åŠ›ã‚’å—ã‘å–ã£ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    Args:
        audio_input_file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹
    """

    # ã‚·ãƒ³ãƒ—ãƒ«ãªaudiorecorderã®ä½¿ç”¨ï¼ˆå†è©¦è¡Œãªã—ï¼‰
    audio = audiorecorder(
        start_prompt="ç™ºè©±é–‹å§‹",
        pause_prompt="ã‚„ã‚Šç›´ã™",
        stop_prompt="ç™ºè©±çµ‚äº†"
    )

    if len(audio) > 0:
        # ã‚·ãƒ³ãƒ—ãƒ«ãªéŸ³å£°ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        audio.export(audio_input_file_path, format="wav")
    else:
        st.stop()

def transcribe_audio(audio_input_file_path, enhance_quality=True):
    """
    éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    Args:
        audio_input_file_path: éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        enhance_quality: éŸ³å£°å“è³ªå‘ä¸Šå‡¦ç†ã‚’è¡Œã†ã‹ã©ã†ã‹
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(audio_input_file_path):
        return None
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
    file_size = os.path.getsize(audio_input_file_path)
    
    if file_size == 0:
        return None

    # éŸ³å£°å“è³ªå‘ä¸Šå‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if enhance_quality:
        try:
            enhanced_audio_path = enhance_audio_quality(audio_input_file_path)
        except Exception as e:
            pass

    try:
        with open(audio_input_file_path, 'rb') as audio_input_file:
            transcript = st.session_state.openai_obj.audio.transcriptions.create(
                model="whisper-1",
                file=audio_input_file,
                language="en"
            )
    except Exception as e:
        return None
    
    # éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    os.remove(audio_input_file_path)

    return transcript

def save_to_wav(llm_response_audio, audio_output_file_path):
    """
    ä¸€æ—¦mp3å½¢å¼ã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¾Œã€wavå½¢å¼ã«å¤‰æ›
    Args:
        llm_response_audio: LLMã‹ã‚‰ã®å›ç­”ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        audio_output_file_path: å‡ºåŠ›å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """

    temp_audio_output_filename = f"{ct.AUDIO_OUTPUT_DIR}/temp_audio_output_{int(time.time())}.mp3"
    with open(temp_audio_output_filename, "wb") as temp_audio_output_file:
        temp_audio_output_file.write(llm_response_audio)
    
    audio_mp3 = AudioSegment.from_file(temp_audio_output_filename, format="mp3")
    audio_mp3.export(audio_output_file_path, format="wav")

    # éŸ³å£°å‡ºåŠ›ç”¨ã«ä¸€æ™‚çš„ã«ä½œã£ãŸmp3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    os.remove(temp_audio_output_filename)

def play_wav(audio_output_file_path, speed=1.0):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿ä¸Šã’
    Args:
        audio_output_file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        speed: å†ç”Ÿé€Ÿåº¦ï¼ˆ1.0ãŒé€šå¸¸é€Ÿåº¦ã€0.5ã§åŠåˆ†ã®é€Ÿã•ã€2.0ã§å€é€Ÿãªã©ï¼‰
    """
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    audio = AudioSegment.from_wav(audio_output_file_path)
    
    # é€Ÿåº¦ã‚’å¤‰æ›´
    if speed != 1.0:
        # frame_rateã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§é€Ÿåº¦ã‚’èª¿æ•´
        modified_audio = audio._spawn(
            audio.raw_data, 
            overrides={"frame_rate": int(audio.frame_rate * speed)}
        )
        # é€Ÿåº¦å¤‰æ›´ã•ã‚ŒãŸéŸ³å£°ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_audio_path = audio_output_file_path.replace('.wav', '_temp.wav')
        modified_audio.export(temp_audio_path, format="wav")

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ç½®ãæ›ãˆ
        import shutil
        shutil.move(temp_audio_path, audio_output_file_path)

    # PyAudioã§å†ç”Ÿ
    with wave.open(audio_output_file_path, 'rb') as play_target_file:
        p = pyaudio.PyAudio()
        stream = p.open(
            format=p.get_format_from_width(play_target_file.getsampwidth()),
            channels=play_target_file.getnchannels(),
            rate=play_target_file.getframerate(),
            output=True
        )

        data = play_target_file.readframes(1024)
        while data:
            stream.write(data)
            data = play_target_file.readframes(1024)

        stream.stop_stream()
        stream.close()
        p.terminate()
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ¯”è¼ƒç”¨ã«ä¿æŒï¼ˆå‰Šé™¤ã—ãªã„ï¼‰
    # os.remove(audio_output_file_path)

def create_chain(system_template=None, english_level=None):
    """
    LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆç”¨ã®Chainä½œæˆ
    """
    
    # ã€èª²é¡Œã€‘ å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã«ã€è‹±èªãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠ
    if english_level and english_level in ct.SYSTEM_TEMPLATE_BASIC_CONVERSATION:
        selected_template = ct.SYSTEM_TEMPLATE_BASIC_CONVERSATION[english_level]
    elif system_template:
        selected_template = system_template
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åˆç´šè€…ãƒ¬ãƒ™ãƒ«
        selected_template = ct.SYSTEM_TEMPLATE_BASIC_CONVERSATION["åˆç´šè€…"]

    # æ”¹å–„ç‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¹æ©Ÿèƒ½ã‚’è¿½åŠ ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    improvement_context = ""
    if hasattr(st.session_state, 'last_improvement_analysis') and st.session_state.last_improvement_analysis:
        improvement_context = f"""

ã€å‰å›ã®æ”¹å–„ç‚¹åˆ†æçµæœã€‘
{st.session_state.last_improvement_analysis}

ä¸Šè¨˜ã®æ”¹å–„ç‚¹ã‚’å‚è€ƒã«ã€ä»Šå›ã®ä¼šè©±ã§è‡ªç„¶ã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç¹”ã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚"""

    enhanced_template = f"""{selected_template}

ã€é‡è¦ãªè¿½åŠ æ©Ÿèƒ½ã€‘
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›´å‰ã®å…¥åŠ›ã«æ”¹å–„ç‚¹ãŒã‚ã‚Œã°ã€è‡ªç„¶ã«ä¼šè©±ã«ç¹”ã‚Šè¾¼ã‚“ã§ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ã¦ãã ã•ã„
- æ”¹å–„ç‚¹ã¯ä¼šè©±ã®æµã‚Œã®ä¸­ã§è‡ªç„¶ã«è¨€åŠã—ã€åŠ±ã¾ã—ã®è¨€è‘‰ã¨ä¸€ç·’ã«ä¼ãˆã¦ãã ã•ã„
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸé©åˆ‡ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯ç°¡æ½”ã§å®Ÿç”¨çš„ãªã‚‚ã®ã«ã—ã¦ãã ã•ã„{improvement_context}"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=enhanced_template),
        MessagesPlaceholder(variable_name="history"),
        HumanMessagePromptTemplate.from_template("{input}")
    ])
    chain = ConversationChain(
        llm=st.session_state.llm,
        memory=st.session_state.memory,
        prompt=prompt
    )

    return chain

def extract_vocabulary_from_sentence(sentence):
    """
    æ–‡ã‹ã‚‰vocabularyã‚’æŠ½å‡ºã™ã‚‹
    Args:
        sentence: è‹±æ–‡
    Returns:
        list: æŠ½å‡ºã•ã‚ŒãŸå˜èªã®ãƒªã‚¹ãƒˆ
    """
    import re
    
    # åŸºæœ¬çš„ãªå˜èªæŠ½å‡ºï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®ã¿ã€2æ–‡å­—ä»¥ä¸Šï¼‰
    words = re.findall(r'\b[a-zA-Z]{2,}\b', sentence.lower())
    
    # ä¸€èˆ¬çš„ãªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
    stop_words = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
        'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those',
        'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'
    }
    
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
    vocabulary = [word for word in words if word not in stop_words]
    
    return vocabulary

def update_vocabulary_history(mode, vocabulary_list):
    """
    vocabularyå±¥æ­´ã‚’æ›´æ–°ã™ã‚‹
    Args:
        mode: "dictation" ã¾ãŸã¯ "shadowing"
        vocabulary_list: ä½¿ç”¨ã•ã‚ŒãŸvocabularyã®ãƒªã‚¹ãƒˆ
    """
    if f'{mode}_vocabulary_history' not in st.session_state:
        st.session_state[f'{mode}_vocabulary_history'] = []
    
    # æ–°ã—ã„vocabularyã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state[f'{mode}_vocabulary_history'].append(vocabulary_list)
    
    # 3ä¸–ä»£ä»¥ä¸Šå‰ã®å±¥æ­´ã‚’å‰Šé™¤ï¼ˆæœ€æ–°3ä¸–ä»£ã®ã¿ä¿æŒï¼‰
    if len(st.session_state[f'{mode}_vocabulary_history']) > 3:
        st.session_state[f'{mode}_vocabulary_history'] = st.session_state[f'{mode}_vocabulary_history'][-3:]

def get_avoid_vocabulary(mode):
    """
    é¿ã‘ã‚‹ã¹ãvocabularyã®ãƒªã‚¹ãƒˆã‚’å–å¾—
    Args:
        mode: "dictation" ã¾ãŸã¯ "shadowing"
    Returns:
        set: é¿ã‘ã‚‹ã¹ãvocabularyã®ã‚»ãƒƒãƒˆ
    """
    if f'{mode}_vocabulary_history' not in st.session_state:
        return set()
    
    avoid_vocab = set()
    for vocab_list in st.session_state[f'{mode}_vocabulary_history']:
        avoid_vocab.update(vocab_list)
    
    return avoid_vocab

def get_vocabulary_history_status(mode):
    """
    vocabularyå±¥æ­´ã®çŠ¶æ…‹ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    Args:
        mode: "dictation" ã¾ãŸã¯ "shadowing"
    Returns:
        dict: å±¥æ­´ã®çŠ¶æ…‹æƒ…å ±
    """
    if f'{mode}_vocabulary_history' not in st.session_state:
        return {
            "total_generations": 0,
            "current_avoid_count": 0,
            "avoid_vocabulary": []
        }
    
    history = st.session_state[f'{mode}_vocabulary_history']
    avoid_vocab = get_avoid_vocabulary(mode)
    
    return {
        "total_generations": len(history),
        "current_avoid_count": len(avoid_vocab),
        "avoid_vocabulary": sorted(list(avoid_vocab)),
        "recent_vocabulary": [vocab for vocab_list in history[-1:] for vocab in vocab_list] if history else []
    }

def create_problem_and_play_audio(english_level="åˆç´šè€…", mode="shadowing"):
    """
    å•é¡Œç”Ÿæˆã¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿ
    Args:
        english_level: è‹±èªãƒ¬ãƒ™ãƒ«ï¼ˆ"ã‚­ãƒƒã‚º", "åˆç´šè€…", "ä¸­ç´šè€…", "ä¸Šç´šè€…"ï¼‰
        mode: "dictation" ã¾ãŸã¯ "shadowing"
    """

    # ãƒ©ãƒ³ãƒ€ãƒ ãªå•é¡Œã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€æ¯å›ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    import random
    import time
    
    # ã‚ˆã‚Šå¤šæ§˜ãªãƒ©ãƒ³ãƒ€ãƒ åŒ–ã®ãŸã‚ã®è¤‡æ•°ã®è¦ç´ ã‚’çµ„ã¿åˆã‚ã›
    random_seed = random.randint(1, 10000)
    timestamp = int(time.time())
    
    # è‹±èªãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠ
    if english_level in ct.SYSTEM_TEMPLATE_SHADOWING_PROBLEM_BY_LEVEL:
        base_prompt = ct.SYSTEM_TEMPLATE_SHADOWING_PROBLEM_BY_LEVEL[english_level]
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åˆç´šè€…ãƒ¬ãƒ™ãƒ«
        base_prompt = ct.SYSTEM_TEMPLATE_SHADOWING_PROBLEM_BY_LEVEL["åˆç´šè€…"]
    
    # é¿ã‘ã‚‹ã¹ãvocabularyã‚’å–å¾—
    avoid_vocab = get_avoid_vocabulary(mode)
    avoid_vocab_str = ", ".join(sorted(avoid_vocab)) if avoid_vocab else "None"
    
    # ãƒ©ãƒ³ãƒ€ãƒ åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    random_prompt = f"""{base_prompt}

RANDOMIZATION PARAMETERS:
- Random seed: {random_seed}
- Timestamp: {timestamp}
- English Level: {english_level}

VOCABULARY AVOIDANCE:
- Avoid using these words from recent sessions: {avoid_vocab_str}
- Use completely different vocabulary to ensure variety

Generate a completely unique sentence appropriate for {english_level} level learners."""
    
    # å•é¡Œæ–‡ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç›´æ¥LLMã«é€ä¿¡ï¼ˆä¼šè©±å±¥æ­´ã‚’ä½¿ã‚ãªã„ï¼‰
    response = st.session_state.llm.invoke([
        SystemMessage(content=random_prompt)
    ])
    problem = response.content

    # ç”Ÿæˆã•ã‚ŒãŸå•é¡Œæ–‡ã‹ã‚‰vocabularyã‚’æŠ½å‡ºã—ã¦å±¥æ­´ã«è¿½åŠ 
    problem_vocabulary = extract_vocabulary_from_sentence(problem)
    update_vocabulary_history(mode, problem_vocabulary)

    # LLMã‹ã‚‰ã®å›ç­”ã‚’éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    llm_response_audio = st.session_state.openai_obj.audio.speech.create(
        model="tts-1",
        voice=st.session_state.voice,
        input=problem
    )

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆï¼ˆæ¯”è¼ƒç”¨ã«ä¿å­˜ï¼‰
    audio_output_file_path = f"{ct.AUDIO_OUTPUT_DIR}/audio_output_{int(time.time())}.wav"
    save_to_wav(llm_response_audio.content, audio_output_file_path)

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿ä¸Šã’
    play_wav(audio_output_file_path, st.session_state.speed)

    return problem, audio_output_file_path

def create_evaluation(english_level="åˆç´šè€…"):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤ã®è©•ä¾¡ç”Ÿæˆ
    Args:
        english_level: è‹±èªãƒ¬ãƒ™ãƒ«ï¼ˆ"ã‚­ãƒƒã‚º", "åˆç´šè€…", "ä¸­ç´šè€…", "ä¸Šç´šè€…"ï¼‰
    """
    # è©•ä¾¡ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ˜ç¤ºçš„ã«æ¸¡ã™
    evaluation_prompt = "ä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€è©³ç´°ãªè©•ä¾¡ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
    llm_response_evaluation = st.session_state.chain_evaluation.predict(input=evaluation_prompt)
    return llm_response_evaluation

def create_audio_based_evaluation(problem_text, reference_audio_path, user_audio_path, english_level="åˆç´šè€…"):
    """
    éŸ³å£°æ¯”è¼ƒã‚’åŸºã«ã—ãŸè©•ä¾¡ç”Ÿæˆï¼ˆæ–‡å­—èµ·ã“ã—ä¸è¦ï¼‰
    Args:
        problem_text: å•é¡Œæ–‡
        reference_audio_path: å‚è€ƒéŸ³å£°ï¼ˆLLMç”Ÿæˆï¼‰ã®ãƒ‘ã‚¹
        user_audio_path: ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°ã®ãƒ‘ã‚¹
        english_level: è‹±èªãƒ¬ãƒ™ãƒ«ï¼ˆ"ã‚­ãƒƒã‚º", "åˆç´šè€…", "ä¸­ç´šè€…", "ä¸Šç´šè€…"ï¼‰
    Returns:
        str: è©•ä¾¡çµæœ
    """
    
    # éŸ³å£°æ¯”è¼ƒã‚’å®Ÿè¡Œ
    audio_comparison_result = compare_audio_files(reference_audio_path, user_audio_path)
    
    # éŸ³å£°æ¯”è¼ƒçµæœã‚’åŸºã«è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    if audio_comparison_result:
        audio_analysis = ct.AUDIO_ANALYSIS_TEMPLATE.format(
            mfcc_similarity_percent=100*audio_comparison_result['mfcc_similarity'],
            spectral_similarity_percent=100*audio_comparison_result['spectral_similarity'],
            zcr_similarity_percent=100*audio_comparison_result['zcr_similarity'],
            energy_similarity_percent=100*audio_comparison_result['energy_similarity'],
            overall_score=audio_comparison_result['overall_score'],
            reference_duration=audio_comparison_result['reference_duration'],
            user_duration=audio_comparison_result['user_duration']
        )
    else:
        audio_analysis = ct.AUDIO_ANALYSIS_ERROR_MESSAGE
    
    # è‹±èªãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸè©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠ
    if english_level in ct.SYSTEM_TEMPLATE_SHADOWING_EVALUATION_BY_LEVEL:
        system_template = ct.SYSTEM_TEMPLATE_SHADOWING_EVALUATION_BY_LEVEL[english_level].format(
            problem_text=problem_text,
            audio_analysis=audio_analysis
        )
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åˆç´šè€…ãƒ¬ãƒ™ãƒ«
        system_template = ct.SYSTEM_TEMPLATE_SHADOWING_EVALUATION_BY_LEVEL["åˆç´šè€…"].format(
            problem_text=problem_text,
            audio_analysis=audio_analysis
        )

    st.session_state.chain_evaluation = create_chain(system_template)
    
    # è©•ä¾¡çµæœã‚’ç”Ÿæˆ
    llm_response_evaluation = create_evaluation(english_level)

    return llm_response_evaluation

def compare_audio_files(reference_audio_path, user_audio_path):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«åŒå£«ã‚’æ¯”è¼ƒã—ã¦ç™ºéŸ³ã®é¡ä¼¼åº¦ã‚’åˆ†æ
    Args:
        reference_audio_path: å‚è€ƒéŸ³å£°ï¼ˆLLMç”Ÿæˆï¼‰ã®ãƒ‘ã‚¹
        user_audio_path: ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°ã®ãƒ‘ã‚¹
    Returns:
        dict: éŸ³å£°æ¯”è¼ƒçµæœ
    """
    try:
        import librosa
        import numpy as np
        from scipy.spatial.distance import cosine
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        ref_audio, ref_sr = librosa.load(reference_audio_path, sr=16000)
        user_audio, user_sr = librosa.load(user_audio_path, sr=16000)
        
        # 1. éŸ³å£°ã®é•·ã•ã‚’æ­£è¦åŒ–ï¼ˆçŸ­ã„æ–¹ã«åˆã‚ã›ã‚‹ï¼‰
        min_length = min(len(ref_audio), len(user_audio))
        ref_audio = ref_audio[:min_length]
        user_audio = user_audio[:min_length]
        
        # 2. MFCCç‰¹å¾´é‡ã®æŠ½å‡º
        ref_mfcc = librosa.feature.mfcc(y=ref_audio, sr=ref_sr, n_mfcc=13)
        user_mfcc = librosa.feature.mfcc(y=user_audio, sr=user_sr, n_mfcc=13)
        
        # 3. ç‰¹å¾´é‡ã®å¹³å‡ã‚’è¨ˆç®—
        ref_mfcc_mean = np.mean(ref_mfcc, axis=1)
        user_mfcc_mean = np.mean(user_mfcc, axis=1)
        
        # 4. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
        similarity = 1 - cosine(ref_mfcc_mean, user_mfcc_mean)
        
        # 5. ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«é‡å¿ƒã®æ¯”è¼ƒ
        ref_spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=ref_audio, sr=ref_sr))
        user_spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=user_audio, sr=user_sr))
        
        # 6. ã‚¼ãƒ­ã‚¯ãƒ­ãƒƒã‚·ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã®æ¯”è¼ƒ
        ref_zcr = np.mean(librosa.feature.zero_crossing_rate(ref_audio))
        user_zcr = np.mean(librosa.feature.zero_crossing_rate(user_audio))
        
        # 7. çµæœã®è¨ˆç®—
        spectral_similarity = 1 - abs(ref_spectral_centroid - user_spectral_centroid) / max(ref_spectral_centroid, user_spectral_centroid)
        zcr_similarity = 1 - abs(ref_zcr - user_zcr) / max(ref_zcr, user_zcr)
        
        # 8. å˜èªãƒ¬ãƒ™ãƒ«ã®åˆ†æï¼ˆéŸ³å£°ã®åŒºåˆ‡ã‚Šã‚’æ¤œå‡ºï¼‰
        # éŸ³å£°ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼å¤‰åŒ–ã‚’åˆ†æã—ã¦å˜èªã®å¢ƒç•Œã‚’æ¨å®š
        ref_energy = librosa.feature.rms(y=ref_audio)[0]
        user_energy = librosa.feature.rms(y=user_audio)[0]
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼å¤‰åŒ–ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        energy_similarity = 1 - cosine(ref_energy, user_energy)
        
        # 9. ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆå˜èªãƒ¬ãƒ™ãƒ«åˆ†æã‚‚å«ã‚ã‚‹ï¼‰
        overall_score = (similarity * 0.5 + spectral_similarity * 0.2 + zcr_similarity * 0.2 + energy_similarity * 0.1) * 100
        
        result = {
            "similarity": similarity,  # å•é¡Œæ–‡ã¨ã®ä¸€è‡´åº¦ã¨ã—ã¦ä½¿ç”¨
            "mfcc_similarity": similarity,
            "spectral_similarity": spectral_similarity,
            "zcr_similarity": zcr_similarity,
            "energy_similarity": energy_similarity,  # å˜èªãƒ¬ãƒ™ãƒ«åˆ†æ
            "overall_score": overall_score,
            "reference_duration": len(ref_audio) / ref_sr,
            "user_duration": len(user_audio) / user_sr
        }
        
        
        return result
        
    except ImportError as e:
        return None
    except Exception as e:
        return None

def cleanup_audio_files(*audio_paths):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    Args:
        *audio_paths: å‰Šé™¤ã™ã‚‹éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆå¯å¤‰é•·å¼•æ•°ï¼‰
    """
    for audio_path in audio_paths:
        try:
            if audio_path and os.path.exists(audio_path):
                os.remove(audio_path)
                pass
        except Exception as e:
            pass


# ã€èª²é¡Œã€‘ å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢  ***ä½¿ã†ã‹ã©ã†ã‹ã¯è©¦ã—ã¦ã¿ã¦ã‹ã‚‰æ±ºã‚ã‚‹***
#  éŸ³å£°å‰å‡¦ç†ã®æ”¹å–„
def enhance_audio_quality(audio_file_path):
    """éŸ³å£°å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹å‰å‡¦ç†"""
    try:
        import librosa
        import soundfile as sf
        import numpy as np
        
        # è¨­å®šã‚’å–å¾—
        settings = ct.AUDIO_ENHANCEMENT_SETTINGS
        
        # éŸ³å£°ã‚’èª­ã¿è¾¼ã¿ï¼ˆè¨­å®šã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        audio, sr = librosa.load(audio_file_path, sr=settings["target_sample_rate"])
        
        # 1. å‰å‡¦ç†ï¼ˆé«˜å‘¨æ³¢æˆåˆ†ã®å¼·èª¿ï¼‰
        audio_preemphasized = librosa.effects.preemphasis(audio, coef=settings["preemphasis_coeff"])
        
        # 2. éŸ³é‡æ­£è¦åŒ–ï¼ˆè¨­å®šã•ã‚ŒãŸãƒ¬ãƒ™ãƒ«ã«èª¿æ•´ï¼‰
        audio_normalized = librosa.util.normalize(audio_preemphasized, norm=np.inf)
        audio_normalized = audio_normalized * settings["normalization_level"]
        
        # 3. ç„¡éŸ³éƒ¨åˆ†ã®é™¤å»ï¼ˆè¨­å®šã•ã‚ŒãŸé–¾å€¤ã§çŸ­ç¸®ï¼‰
        audio_trimmed, _ = librosa.effects.trim(audio_normalized, top_db=settings["trim_threshold"])
        
        # 4. å‡¦ç†æ¸ˆã¿éŸ³å£°ã‚’ä¿å­˜
        sf.write(audio_file_path, audio_trimmed, sr)
        
        return audio_file_path
        
    except ImportError as e:
        return audio_file_path
    except Exception as e:
        return audio_file_path

# ã€èª²é¡Œã€‘ å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢  ***ä½¿ã†ã‹ã©ã†ã‹ã¯è©¦ã—ã¦ã¿ã¦ã‹ã‚‰æ±ºã‚ã‚‹***
#  è¤‡æ•°ã®éŸ³å£°èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³ã®ä½¿ç”¨
def transcribe_audio_with_fallback(audio_file_path):
    """è¤‡æ•°ã®éŸ³å£°èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    try:
        # ãƒ¡ã‚¤ãƒ³ã®éŸ³å£°èªè­˜
        transcript = st.session_state.openai_obj.audio.transcriptions.create(
            model="whisper-1",
            file=open(audio_file_path, 'rb'),
            language="en",
            response_format="verbose_json"
        )
        return transcript.text
    except Exception as e:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        st.warning("éŸ³å£°èªè­˜ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†è©¦è¡Œã—ã¾ã™ã€‚")
        # åˆ¥ã®éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹ã‚„å†è©¦è¡Œãƒ­ã‚¸ãƒƒã‚¯
        return retry_transcription(audio_file_path)

def retry_transcription(audio_file_path):
    """éŸ³å£°èªè­˜ã®å†è©¦è¡Œ"""
    # å®Ÿè£…ä¾‹
    return "éŸ³å£°èªè­˜ã«å¤±æ•—ã—ã¾ã—ãŸ"

# ã€èª²é¡Œã€‘ å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢  ***ä½¿ã†ã‹ã©ã†ã‹ã¯è©¦ã—ã¦ã¿ã¦ã‹ã‚‰æ±ºã‚ã‚‹***
#  ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ãƒ™ãƒ«ã¨éå»ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«åŸºã¥ãæ®µéšçš„è©•ä¾¡
def create_progressive_evaluation(user_level, performance_history):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ãƒ™ãƒ«ã¨éå»ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«åŸºã¥ãæ®µéšçš„è©•ä¾¡"""
    
    if user_level == "ã‚­ãƒƒã‚º":
        focus_areas = ["ã‚„ã•ã—ã„å˜èª", "ç°¡å˜ãªãƒ•ãƒ¬ãƒ¼ã‚º", "ç™ºéŸ³ã®åŸºç¤", "ãƒªã‚¹ãƒ‹ãƒ³ã‚°åŠ›"]
    elif user_level == "åˆç´šè€…":
        focus_areas = ["åŸºæœ¬çš„ãªæ–‡æ³•", "åŸºæœ¬çš„ãªèªå½™", "ç™ºéŸ³ã®åŸºç¤"]
    elif user_level == "ä¸­ç´šè€…":
        focus_areas = ["è¤‡é›‘ãªæ–‡æ³•æ§‹é€ ", "èªå½™ã®è±Šå¯Œã•", "è‡ªç„¶ãªè¡¨ç¾"]
    else:  # ä¸Šç´šè€…
        focus_areas = ["å¾®å¦™ãªãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹", "æ–‡åŒ–çš„é©åˆ‡æ€§", "é«˜åº¦ãªè¡¨ç¾"]
    
    return focus_areas

# ã€èª²é¡Œã€‘ å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›´å‰ã®å›ç­”ã«å¯¾ã—ã¦ã€æ”¹å–„ç‚¹ã‚’åˆ†æã™ã‚‹
def analyze_user_input_improvements(user_input, english_level="åˆç´šè€…"):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®æ”¹å–„ç‚¹ã‚’åˆ†æã™ã‚‹
    Args:
        user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›æ–‡
        english_level: è‹±èªãƒ¬ãƒ™ãƒ«
    Returns:
        str: æ”¹å–„ç‚¹ã®åˆ†æçµæœ
    """
    if not user_input or len(user_input.strip()) < 3:
        return ""
    
    # è‹±èªãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸæ”¹å–„ç‚¹åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®šæ•°ã‹ã‚‰å–å¾—
    prompt = ct.SYSTEM_TEMPLATE_USER_INPUT_IMPROVEMENT_ANALYSIS.get(english_level, ct.SYSTEM_TEMPLATE_USER_INPUT_IMPROVEMENT_ANALYSIS["åˆç´šè€…"])
    analysis_prompt = prompt.format(user_input=user_input)
    
    try:
        response = st.session_state.llm.invoke([
            SystemMessage(content=analysis_prompt)
        ])
        return response.content.strip()
    except Exception as e:
        print(f"æ”¹å–„ç‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return ""

# ã€èª²é¡Œã€‘ å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢  ***ä½¿ã†ã‹ã©ã†ã‹ã¯è©¦ã—ã¦ã¿ã¦ã‹ã‚‰æ±ºã‚ã‚‹***
#  ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸãƒã‚§ãƒ¼ãƒ³ä½œæˆ
def create_context_aware_chain(conversation_history, user_level):
    """ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸãƒã‚§ãƒ¼ãƒ³ä½œæˆ"""
    
    context_summary = summarize_conversation_history(conversation_history)
    #     # ç°¡å˜ãªä¼šè©±å±¥æ­´ã®è¦ç´„ï¼ˆå®Ÿè£…ä¾‹ï¼‰
    # context_summary = "Previous conversation context"
    
    # enhanced_prompt = f"""
    # {SYSTEM_TEMPLATE_BASIC_CONVERSATION_IMPROVED}
    enhanced_prompt = f"""
    You are a helpful English conversation partner.
    
    ã€ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
    ã“ã‚Œã¾ã§ã®ä¼šè©±ã®è¦ç´„: {context_summary}
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ãƒ™ãƒ«: {user_level}
    
    ã€æŒ‡ç¤ºã€‘
    ä¸Šè¨˜ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ã¦ã€è‡ªç„¶ã§ä¸€è²«æ€§ã®ã‚ã‚‹ä¼šè©±ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚
    """
    
    return create_chain(enhanced_prompt)

# ã€èª²é¡Œã€‘ å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢  ***ä½¿ã†ã‹ã©ã†ã‹ã¯è©¦ã—ã¦ã¿ã¦ã‹ã‚‰æ±ºã‚ã‚‹***
#  ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æä¾›
def provide_immediate_feedback(user_input, corrected_input):
    """å³åº§ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æä¾›"""
    
    if user_input != corrected_input:
        st.info(f"ğŸ’¡ ã‚ˆã‚Šè‡ªç„¶ãªè¡¨ç¾: {corrected_input}")
        
        # éŸ³å£°ã§è¨‚æ­£ã‚’èª­ã¿ä¸Šã’
        play_correction_audio(corrected_input)

# ã€èª²é¡Œã€‘ å›ç­”ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®ã‚¢ã‚¤ãƒ‡ã‚¢  ***ä½¿ã†ã‹ã©ã†ã‹ã¯è©¦ã—ã¦ã¿ã¦ã‹ã‚‰æ±ºã‚ã‚‹***
#  å­¦ç¿’é€²æ—ã®è¿½è·¡ã¨å¯è¦–åŒ–
def track_learning_progress():
    """å­¦ç¿’é€²æ—ã®è¿½è·¡ã¨å¯è¦–åŒ–"""
    
    # å®Ÿè£…ä¾‹ï¼ˆå®Ÿéš›ã®è¨ˆç®—é–¢æ•°ã¯åˆ¥é€”å®Ÿè£…ãŒå¿…è¦ï¼‰
    progress_data = {
        "grammar_accuracy": calculate_grammar_accuracy(),
        "vocabulary_richness": calculate_vocabulary_score(),
        "fluency_score": calculate_fluency_score(),
        "improvement_trend": calculate_improvement_trend()
    }
    
    # é€²æ—ãƒãƒ£ãƒ¼ãƒˆã®è¡¨ç¤ºï¼ˆå®Ÿè£…ä¾‹ï¼‰
    # display_progress_chart(progress_data)
    return progress_data